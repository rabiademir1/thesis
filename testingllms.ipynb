{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sig90QDOHGHz"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbPDn77iHSfF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('Train_data.csv')\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmnoyDQQxPKb"
      },
      "outputs": [],
      "source": [
        "x = df[\"label\"]\n",
        "y = df[\"text\"]\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=42, stratify=x)\n",
        "train_df = pd.DataFrame({\"label\": x_train, \"text\": y_train})\n",
        "test_df = pd.DataFrame({\"label\": x_test, \"text\": y_test})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9RxuQluNLZg"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Use BioBERT (or replace with 'distilbert-base-uncased' for general BERT)\n",
        "# dmis-lab/biobert-base-cased-v1.1\n",
        "# dmis-lab/biobert-base-cased-v1.2\n",
        "model_name = \"dmis-lab/biobert-base-cased-v1.2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Apply tokenization\n",
        "train_df[\"text\"] = train_df[\"text\"].astype(str)\n",
        "test_df[\"text\"] = test_df[\"text\"].astype(str)\n",
        "\n",
        "train_encodings = tokenizer(list(train_df[\"text\"]), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(list(test_df[\"text\"]), truncation=True, padding=True, max_length=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jWr3znVO6w3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_df[\"label\"])\n",
        "test_labels = label_encoder.transform(test_df[\"label\"])\n",
        "\n",
        "# Check mapping of diseases to numbers\n",
        "label_mapping = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))\n",
        "print(\"Label Mapping:\", label_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-iCRrS0PMkS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class DiseaseDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "# Create PyTorch dataset\n",
        "train_dataset = DiseaseDataset(train_encodings, train_labels)\n",
        "test_dataset = DiseaseDataset(test_encodings, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WA2jy-Klfq6O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSe-X6YjPa4U"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# Load model for classification\n",
        "num_labels = len(label_encoder.classes_)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=90\n",
        ")\n",
        "\n",
        "# Create Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XWdppVoh-W6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract log history\n",
        "history = trainer.state.log_history\n",
        "\n",
        "# Separate training and validation losses\n",
        "train_losses = [entry[\"loss\"] for entry in history if \"loss\" in entry]\n",
        "eval_losses = [entry[\"eval_loss\"] for entry in history if \"eval_loss\" in entry]\n",
        "\n",
        "# Create x-axis values (logging steps for training loss, epochs for eval loss)\n",
        "train_steps = [i * training_args.logging_steps for i in range(len(train_losses))]\n",
        "eval_epochs = list(range(1, len(eval_losses) + 1))\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_steps, train_losses, label=\"Training Loss\", marker=\"o\")\n",
        "plt.plot(eval_epochs, eval_losses, label=\"Validation Loss\", marker=\"s\", linestyle=\"dashed\")\n",
        "plt.xlabel(\"Steps (Training) / Epochs (Validation)\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8T2KOwzqPdlv"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Get predictions\n",
        "predictions = trainer.predict(test_dataset)\n",
        "preds = torch.argmax(torch.tensor(predictions.predictions), axis=1)\n",
        "\n",
        "# Convert predictions back to disease labels\n",
        "predicted_labels = label_encoder.inverse_transform(preds)\n",
        "\n",
        "# Print accuracy and classification report\n",
        "accuracy = accuracy_score(test_df[\"label\"], predicted_labels)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_report(test_df[\"label\"], predicted_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AukdduDIhGX7"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def predict_disease(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)  # Move to GPU if available\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
        "    return predicted_class\n",
        "\n",
        "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
        "results_dict = {}\n",
        "\n",
        "for i, symptom_text in enumerate(y_test):\n",
        "  predicted_label = predict_disease(symptom_text)\n",
        "  predicted_disease = reverse_label_mapping.get(predicted_label, \"Unknown Disease\")\n",
        "  actual_disease = x_test.iloc[i]\n",
        "\n",
        "  results_dict[i] = {\"Symptom Description\": symptom_text,\n",
        "        \"Predicted Disease\": predicted_disease,\n",
        "        \"Actual Disease\": actual_disease}\n",
        "\n",
        "# Print the dictionary\n",
        "for key, value in results_dict.items():\n",
        "    print(f\"Sample {key}:\")\n",
        "    print(f\"  Symptoms: {value['Symptom Description']}\")\n",
        "    print(f\"  Predicted: {value['Predicted Disease']}\")\n",
        "    print(f\"  Actual: {value['Actual Disease']}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMW5NQKgqf1K"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}